{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\Sudarshan\\Desktop\\files for project\"\n",
    "pdfpages = []\n",
    "chunk_size = 200\n",
    "chunk_overlap = 50 \n",
    "text_chunks = []\n",
    "for file in os.listdir(folder_path):\n",
    "    file_path = os.path.join (folder_path , file)\n",
    "    with open(file_path, 'rb') as f :\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        num_pages = len(reader.pages)\n",
    "        for page_number in range(num_pages):\n",
    "            page = reader.pages[page_number]\n",
    "            text= page.extract_text()\n",
    "            cleaned_text = text.replace('\\n', ' ').strip()\n",
    "            page_data = {\n",
    "                \"document_name\":file,\n",
    "                \"page_number\":page_number+1,\n",
    "                \"text\": cleaned_text\n",
    "            }\n",
    "            pdfpages.append(page_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_data in pdfpages:\n",
    "    text = page_data[\"text\"]\n",
    "    words = text.split()\n",
    "\n",
    "    #here we run chunking loop inside the page loop because we dont want the words of two files mix with eachother \n",
    "    for i in range (0 , len(words),chunk_size-chunk_overlap):\n",
    "        chunk_words= words [i:i+chunk_size]\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "        text_chunks.append({\n",
    "            \"document_name\": page_data[\"document_name\"],\n",
    "            \"page_number\": page_data[\"page_number\"],\n",
    "            \"chunk_index\":(i//(chunk_size-chunk_overlap))+1,\n",
    "            \"text\":chunk_text  \n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 14/14 [00:07<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') #384 dimensional encoding per chunk \n",
    "texts = [chunk[\"text\"] for chunk in text_chunks ]\n",
    "embeddings = model.encode(\n",
    "    texts , \n",
    "    batch_size=20,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faiss Indexing and Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(271, 384)\n"
     ]
    }
   ],
   "source": [
    "embeddings_array= np.array(embeddings, dtype='float32')\n",
    "embedding_dim = embeddings_array.shape[1]\n",
    "print(embeddings_array.shape)\n",
    "#(271, 384)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = model.encode('what is ai ?')\n",
    "query_embedding = query.reshape(1,-1)\n",
    "k=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=384\n",
    "index= faiss.IndexFlatIP(d)\n",
    "index.is_trained\n",
    "index.add(embeddings_array) \n",
    "distances , indices= index.search(query_embedding, k)\n",
    "print(indices[0])#[248 246  46  21  56 267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_results = [\n",
    "    f\"Rank {rank+1}: Doc='{text_chunks[idx]['document_name']}', \"\n",
    "    f\"Page={text_chunks[idx]['page_number']}, \"\n",
    "    f\"Chunk={text_chunks[idx]['chunk_index']}\\n\"\n",
    "    f\"Text Preview: {text_chunks[idx]['text'][:100]}...\"  # first 100 chars\n",
    "    for rank, idx in enumerate(indices[0])\n",
    "]\n",
    "\n",
    "for i in top_k_results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nbits= 32\n",
    "index = faiss.IndexLSH(d,nbits)\n",
    "distances , indexes =index.search(query_embedding, k)\n",
    "print(indexes[0])\n",
    "#[-1 -1 -1 -1 -1 -1] (vector size is too small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[248 246  46  21  56 267]\n"
     ]
    }
   ],
   "source": [
    "M= 32 \n",
    "ef_search = 200 \n",
    "ef_construction = 100 \n",
    "index= faiss.IndexHNSWFlat(d,M)\n",
    "index.hnsw.efSearch = ef_search\n",
    "index.hnsw.efConstruction = ef_construction\n",
    "index.add(embeddings_array)\n",
    "distances , indexes =index.search(query_embedding, k)\n",
    "print(indexes[0])\n",
    "#[248 246  46  21  56 267]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[248 246  46  56 267 249]\n"
     ]
    }
   ],
   "source": [
    "nlist= 5\n",
    "quantizer= faiss.IndexFlatIP(384)\n",
    "index= faiss.IndexIVFFlat(quantizer , 384 , nlist)\n",
    "index.train(embeddings_array)\n",
    "index.add(embeddings_array)\n",
    "distances , indexes =index.search(query_embedding, k)\n",
    "print(indexes[0])\n",
    "#[248 246  46  56 267 249]\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
