{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31586d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b57d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeeccd9",
   "metadata": {},
   "source": [
    "Tokenization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=spm.SentencePieceProcessor(model_file = 'spm_unigram_en.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1d320",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (2089606159.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef tokenize_text(input_text):\u001b[39m\n                                  ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(input_text):\n",
    "    ids = sp.encode_as_ids(text)\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91489045",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (2574531015.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef token_embedding( token_ids):\u001b[39m\n                                    ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "model_dim = 512\n",
    "vocab_size = sp.get_piece_size()\n",
    "def token_embedding( token_ids):\n",
    "    embedding_matrix = np.random.radn(vocab_size, model_dim)\n",
    "    embeddings = embedding_matrix[token_ids]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385de466",
   "metadata": {},
   "source": [
    "Positional encoding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef58fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(max_sequence_length , dmodel):\n",
    "    positional_encoding_matrix = np.zeros((max_sequence_length , dmodel))\n",
    "    for position in range(max_sequence_length):\n",
    "        for i in range(0,dmodel//2):\n",
    "            positional_encoding_matrix[position , 2*i] = np.sin(position/(10000**(2*i/dmodel)))\n",
    "            positional_encoding_matrix[position , 2*i +1]=  np.cos(position/(10000**(2*i/dmodel)))\n",
    "    return positional_encoding_matrix\n",
    "                                                                              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf884ce",
   "metadata": {},
   "source": [
    "Scaled Dot product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScaledDotProductAttetnion(Q,K,V):\n",
    "    scaled_scores = (np.matmul(Q,K.T))/np.sqrt(Q.shape[-1])\n",
    "    exp_scores = np.exp(scaled_scores - np.max(scaled_scores, axis=-1,keepdims=True)) #subtract the max for numerical stability(Prevents np.exp() from overflowing when numbers are large)\n",
    "    attention_weights = exp_scores / np.sum(exp_scores, axis=-1 ,keepdims=True)\n",
    "    attention_score = np.matmul(attention_weights ,V)\n",
    "    return attention_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdd10a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 3. 2.]\n",
      " [2. 3. 2.]]\n"
     ]
    }
   ],
   "source": [
    "Q = np.array([[2,3,4],\n",
    "               [5,5,6]])\n",
    "\n",
    "K = np.array([[2,35,5],\n",
    "               [5,5,4]])\n",
    "\n",
    "V = np.array([[2,3,2],\n",
    "               [5,4,5]])\n",
    "attn_score = ScaledDotProductAttetnion(Q,K,V)\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57af513f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "[[[ 1  4]\n",
      "  [ 2  5]\n",
      "  [ 3  6]]\n",
      "\n",
      " [[ 7 10]\n",
      "  [ 8 11]\n",
      "  [ 9 12]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arr = np.array([[[1, 2, 3],\n",
    "                 [4, 5, 6]],\n",
    "\n",
    "                [[7, 8, 9],\n",
    "                 [10, 11, 12]]])\n",
    "print(arr.shape)  \n",
    "print(arr.transpose(0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bda3e5b",
   "metadata": {},
   "source": [
    "Tensor Surgery(Splitting Heads ,reshaping , transposing , Combining heads )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbdcd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(tensor , batch_size , heads):\n",
    "    dimension_head = int(tensor.shape[-1]/ heads) #shape[-1] takes the dimension of tensor \n",
    "    sequence_length = tensor.shape[1]\n",
    "    x = tensor.reshape(batch_size , sequence_length , heads , dimension_head)\n",
    "    final_tensor = x.transpose(0 , 2 , 1 , 3)\n",
    "    return final_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ab33c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape :(32, 8, 512)\n",
      "Output  Shape :(32, 8, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "B , L , D = 32 , 10 , 512\n",
    "H=8\n",
    "example_tensor = np.zeros(( B , H , D )) # np.zeros mocks data\n",
    "reshaped_tensor= split_heads(example_tensor,B,H )\n",
    "print(f\"Input Shape :{example_tensor.shape}\")\n",
    "print(f\"Output  Shape :{reshaped_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_heads(tensor):\n",
    "    batch_size = tensor.shape[0]\n",
    "    heads = tensor.shape[1]\n",
    "    sequence_length = tensor.shape[2]\n",
    "    dimension_head = tensor.shape[3]\n",
    "    x = tensor.transpose(0 , 2 , 1 , 3)\n",
    "    combined_tensor = x.reshape(batch_size , sequence_length , heads * dimension_head)\n",
    "    return combined_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22964b",
   "metadata": {},
   "source": [
    "Sub-layers components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_connection( x , sublayer_output_FNN_or_attention):\n",
    "    return x + sublayer_output\n",
    "\n",
    "def layer_normalization (residual_output ,gamma , beta , epsilon = 1e-5 ): #here gamma and beta are learnable parameters (weights and bias)\n",
    "    mean = np.mean( residual_output , axis = -1 , keepdims = True)\n",
    "    st_deviation= np.std( residual_output , axis = -1 , keepdims = True)\n",
    "    normalized_output = ((residual_output -mean)/(st_deviation+epsilon) )*gamma + beta\n",
    "    return normalized_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03e0941",
   "metadata": {},
   "source": [
    "The Feed-Forward Network(FFN)(In tranformer paper , FFN contains two linear layer + RELU activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3952ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(output_from_attention , W1 , B1 , W2 , B2):\n",
    "    linear_transform1 = np.matmul(output_from_attention , W1) + B1\n",
    "    activation_output = np.maximum (0 , linear_transform1) #ReLU activation\n",
    "    linear_transform2 = np.matmul(activation_output , W2) + B2\n",
    "    return linear_transform2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed35c86",
   "metadata": {},
   "source": [
    "Initializing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dmodel , heads , d_ff):\n",
    "    dimension_head = dmodel // heads\n",
    "    parameters = {}\n",
    "    parameters['encodding_matrix'] = \n",
    "    parameters['Wq'] = np.random.randn(dmodel , dmodel) / np.sqrt(dmodel)\n",
    "    parameters['Wk'] = np.random.randn(dmodel , dmodel) / np.sqrt(dmodel)\n",
    "    parameters['Wv'] = np.random.randn(dmodel , dmodel) / np.sqrt(dmodel)\n",
    "    parameters['Wo'] = np.random.randn(dmodel , dmodel) / np.sqrt(dmodel)\n",
    "    parameters['W1'] = np.random.randn(dmodel , d_ff) / np.sqrt(dmodel)\n",
    "    parameters['B1'] = np.zeros((1 , d_ff))\n",
    "    parameters['W2'] = np.random.randn(d_ff , dmodel) / np.sqrt(d_ff)\n",
    "    parameters['B2'] = np.zeros((1 , dmodel))\n",
    "    parameters['gamma1'] = np.ones((1 , dmodel))\n",
    "    parameters['beta1'] = np.zeros((1 , dmodel))\n",
    "    parameters['gamma2'] = np.ones((1 , dmodel))\n",
    "    parameters['beta2'] = np.zeros((1 , dmodel))\n",
    "    return parameters\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f77e007",
   "metadata": {},
   "source": [
    "Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ef0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(input_tensor , parameters , heads):\n",
    "    Wq , Wk , Wv , Wo = parameters['Wq'] , parameters['Wk'] , parameters['Wv'] , parameters['Wo']\n",
    "    W1 , B1 , W2 , B2 = parameters['W1'] , parameters['B1'] , parameters['W2'] , parameters['B2']\n",
    "    gamma1 , beta1 , gamma2 , beta2 = parameters['gamma1'] , parameters['beta1'] , parameters['gamma2'] , parameters['beta2']\n",
    "    Q , K , V = np.matmul(input_tensor , Wq) , np.matmul(input_tensor , Wk) , np.matmul(input_tensor , Wv)\n",
    "    batch_size = input_tensor.shape[0]\n",
    "    Q_heads , K_heads , V_heads = split_heads(Q , batch_size , heads)   , split_heads(K , batch_size , heads) , split_heads(V , batch_size , heads)\n",
    "    attention_output_heads = ScaledDotProductAttetnion(Q_heads , K_heads , V_heads)\n",
    "    combined_attention_output = combine_heads(attention_output_heads)\n",
    "    attention_output = np.matmul(combined_attention_output , Wo)\n",
    "    residual_connection1 = residual_connection(input_tensor , attention_output)\n",
    "    normalized_output1 = layer_normalization(residual_connection1 , gamma1 , beta1)\n",
    "    ffn_output = feed_forward(normalized_output1 , W1 , B1 , W2 , B2)\n",
    "    residual_connection2 = residual_connection(normalized_output1 , ffn_output)\n",
    "    normalized_output2 = layer_normalization(residual_connection2 , gamma2 , beta2)\n",
    "    return normalized_output2\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ccf44",
   "metadata": {},
   "source": [
    "Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36bc0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
