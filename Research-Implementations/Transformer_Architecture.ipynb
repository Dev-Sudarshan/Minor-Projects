{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdce5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf884ce",
   "metadata": {},
   "source": [
    "Scaled Dot product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScaledDotProductAttetnion(Q,K,V):\n",
    "    scaled_scores = (np.matmul(Q,K.T))/np.sqrt(Q.shape[-1])\n",
    "    exp_scores = np.exp(scaled_scores - np.max(scaled_scores, axis=-1,keepdims=True)) #subtract the max for numerical stability(Prevents np.exp() from overflowing when numbers are large)\n",
    "    attention_weights = exp_scores / np.sum(exp_scores, axis=-1 ,keepdims=True)\n",
    "    attention_score = np.matmul(attention_weights ,V)\n",
    "    return attention_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdd10a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 3. 2.]\n",
      " [2. 3. 2.]]\n"
     ]
    }
   ],
   "source": [
    "Q = np.array([[2,3,4],\n",
    "               [5,5,6]])\n",
    "\n",
    "K = np.array([[2,35,5],\n",
    "               [5,5,4]])\n",
    "\n",
    "V = np.array([[2,3,2],\n",
    "               [5,4,5]])\n",
    "attn_score = ScaledDotProductAttetnion(Q,K,V)\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57af513f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "[[[ 1  4]\n",
      "  [ 2  5]\n",
      "  [ 3  6]]\n",
      "\n",
      " [[ 7 10]\n",
      "  [ 8 11]\n",
      "  [ 9 12]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arr = np.array([[[1, 2, 3],\n",
    "                 [4, 5, 6]],\n",
    "\n",
    "                [[7, 8, 9],\n",
    "                 [10, 11, 12]]])\n",
    "print(arr.shape)  \n",
    "print(arr.transpose(0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bda3e5b",
   "metadata": {},
   "source": [
    "Tensor Surgery(Splitting Heads ,reshaping , transposing , Combining heads )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbdcd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(tensor , batch_size , heads):\n",
    "    dimension_head = int(tensor.shape[-1]/ heads) #shape[-1] takes the dimension of tensor \n",
    "    sequence_length = tensor.shape[1]\n",
    "    x = tensor.reshape(batch_size , sequence_length , heads , dimension_head)\n",
    "    final_tensor = x.transpose(0 , 2 , 1 , 3)\n",
    "    return final_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ab33c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape :(32, 8, 512)\n",
      "Output  Shape :(32, 8, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "B , L , D = 32 , 10 , 512\n",
    "H=8\n",
    "example_tensor = np.zeros(( B , H , D )) # np.zeros mocks data\n",
    "reshaped_tensor= split_heads(example_tensor,B,H )\n",
    "print(f\"Input Shape :{example_tensor.shape}\")\n",
    "print(f\"Output  Shape :{reshaped_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_heads(tensor):\n",
    "    batch_size = tensor.shape[0]\n",
    "    heads = tensor.shape[1]\n",
    "    sequence_length = tensor.shape[2]\n",
    "    dimension_head = tensor.shape[3]\n",
    "    x = tensor.transpose(0 , 2 , 1 , 3)\n",
    "    combined_tensor = x.reshape(batch_size , sequence_length , heads * dimension_head)\n",
    "    return combined_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22964b",
   "metadata": {},
   "source": [
    "Sub-layers components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_connection( x , sublayer_output):\n",
    "    return x + sublayer_output\n",
    "\n",
    "def layer_normalization (residual_output ,gamma , beta , epsilon = 1e-5 ): #here gamma and beta are learnable parameters (weights and bias)\n",
    "    mean = np.mean( residual_output , axis = -1 , keepdims = True)\n",
    "    st_deviation= np.std( residual_output , axis = -1 , keepdims = True)\n",
    "    normalized_output = ((residual_output -mean)/(st_deviation+epsilon) )*gamma + beta\n",
    "    return normalized_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03e0941",
   "metadata": {},
   "source": [
    "The Feed-Forward Network(FFN)(In tranformer paper , FFN contains two linear layer + RELU activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3952ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(output_from_attention , W1 , B1 , W2 , B2):\n",
    "    linear_transform1 = np.matmul(output_from_attention , W1) + B1\n",
    "    activation_output = np.maximum (0 , linear_transform1) #ReLU activation\n",
    "    linear_transform2 = np.matmul(activation_output , W2) + B2\n",
    "    return linear_transform2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385de466",
   "metadata": {},
   "source": [
    "Positional encoding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef58fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(max_sequence_length , dmodel):\n",
    "    positional_encoding_matrix = np.zeros((max_sequence_length , dmodel))\n",
    "    for position in range(max_sequence_length):\n",
    "        for i in range(0,dmodel//2):\n",
    "            positional_encoding_matrix[position , 2*i] = np.sin(position/(10000**(2*i/dmodel)))\n",
    "            positional_encoding_matrix[position , 2*i +1]=  np.cos(position/(10000**(2*i/dmodel)))\n",
    "    return positional_encoding_matrix\n",
    "                                                                              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed35c86",
   "metadata": {},
   "source": [
    "Initializing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dmodel , heads , d_ff):\n",
    "    dimension_head = dmodel // heads\n",
    "    parameters = {}\n",
    "    parameters['Wq'] = np.random.randn(dmodel , dmodel) / np.sqrt(dmodel)\n",
    "    parameters['Wk'] = np.random.randn(dmodel , dmodel) / np.sqrt(dmodel)\n",
    "    parameters['Wv'] = np.random.randn(dmodel , dmodel) / np.sqrt(dmodel)\n",
    "    parameters['Wo'] = np.random.randn(dmodel , dmodel) / np.sqrt(dmodel)\n",
    "    parameters['W1'] = np.random.randn(dmodel , d_ff) / np.sqrt(dmodel)\n",
    "    parameters['B1'] = np.zeros((1 , d_ff))\n",
    "    parameters['W2'] = np.random.randn(d_ff , dmodel) / np.sqrt\n",
    "    parameters['B2'] = np.zeros((1 , dmodel))\n",
    "    parameters['gamma'] = np.ones((1 , dmodel))\n",
    "    parameters['beta'] = np.zeros((1 , dmodel))\n",
    "    return parameters\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f77e007",
   "metadata": {},
   "source": [
    "Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ef0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(input_tensor , "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
